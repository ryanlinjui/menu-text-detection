{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to HuggingFace (just login once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Menu Image Datasets\n",
    "- Use metadata.jsonl to store the metadata of the images.\n",
    "- [Google AI Studio](https://aistudio.google.com) or [OpenAI ChatGPT](https://chatgpt.com) by this prompt\n",
    "- Gemini API by doing the function calling. Start the gradio app and input image and access token to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(path=\"datasets/menu-zh-TW\")\n",
    "dataset.push_to_hub(repo_id=\"ryanlinjui/menu-zh-TW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig\n",
    "from menu.donut import DonutDatasets\n",
    "\n",
    "DATASETS_REPO_ID = \"ryanlinjui/menu-zh-TW\"\n",
    "PRETRAINED_MODEL_REPO_ID = \"naver-clova-ix/donut-base\"\n",
    "TASK_PROMPT_NAME = \"<s_menu>\"\n",
    "MAX_LENGTH = 768\n",
    "IMAGE_SIZE = [1280, 960]\n",
    "\n",
    "raw_datasets = load_dataset(DATASETS_REPO_ID)\n",
    "\n",
    "# Config\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(PRETRAINED_MODEL_REPO_ID)\n",
    "config.encoder.image_size = IMAGE_SIZE\n",
    "config.decoder.max_length = MAX_LENGTH\n",
    "\n",
    "# Processor\n",
    "processor = DonutProcessor.from_pretrained(PRETRAINED_MODEL_REPO_ID)\n",
    "processor.feature_extractor.size = IMAGE_SIZE[::-1]\n",
    "processor.feature_extractor.do_align_long_axis = False\n",
    "\n",
    "# DonDatasets\n",
    "datasets = DonutDatasets(\n",
    "    datasets=raw_datasets,\n",
    "    processor=processor,\n",
    "    image_column=\"image\",\n",
    "    annotation_column=\"menu\",\n",
    "    task_start_token=TASK_PROMPT_NAME,\n",
    "    prompt_end_token=TASK_PROMPT_NAME,\n",
    "    train_split=0.8,\n",
    "    validation_split=0.1,\n",
    "    test_split=0.1\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(PRETRAINED_MODEL_REPO_ID, config=config)\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([TASK_PROMPT_NAME])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 3e-5\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "EVALUATION_BATCH_SIZE = 1\n",
    "CHECKPOINT_PATH = \"./.checkpoints\"\n",
    "SEED = 2022\n",
    "WARMUP_STEPS = 30\n",
    "MAX_STEPS = -1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    model.to(\"cuda\")\n",
    "else:\n",
    "    print(\"Using default device\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVALUATION_BATCH_SIZE,\n",
    "    output_dir=CHECKPOINT_PATH,\n",
    "    seed=SEED,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"donut-base-finetuned-menu\"\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"test\"]\n",
    ")\n",
    "trainer.train()\n",
    "'''\n",
    "donut-base-finetuned-menu\n",
    "{\"file_name\":\"94.jpg\",\"ground_truth\":{\"gt_parse\":{\"restaurant\":\"安田麵屋\",\"address\":\"\",\"phone\":\"\",\"business_hours\":\"\",\"items\":[{\"name\":\"豚骨拉麵\",\"price\":\"120\"},{\"name\":\"明太子烏龍麵\",\"price\":\"100\"}]}}}\n",
    "resume_from_checkpoint_path: null # only used for resume_from_checkpoint option in PL\n",
    "result_path: \"./result\"\n",
    "pretrained_model_name_or_path: \"naver-clova-ix/donut-base\" # loading a pre-trained model (from moldehub or path)\n",
    "dataset_name_or_paths: [\"ryanlinjui/donut-menu-zh-TW\"] # loading datasets (from moldehub or path)\n",
    "sort_json_key: False # cord dataset is preprocessed, and publicly available at https://huggingface.co/datasets/naver-clova-ix/cord-v2\n",
    "train_batch_sizes: [4]\n",
    "val_batch_sizes: [1]\n",
    "input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)\n",
    "max_length: 768\n",
    "align_long_axis: False\n",
    "num_nodes: 1\n",
    "seed: 2022\n",
    "lr: 3e-5\n",
    "warmup_steps: 30 # 800/8*30/10, 10%\n",
    "num_training_samples_per_epoch: 80\n",
    "max_epochs: 30\n",
    "max_steps: -1\n",
    "num_workers: 8\n",
    "val_check_interval: 1.0\n",
    "check_val_every_n_epoch: 1\n",
    "gradient_clip_val: 1.0\n",
    "verbose: True\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "import re\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/root/menu-text-detection/examples/menu-hd.jpg\").convert(\"RGB\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\".checkpoint/checkpoint-2000\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "pixel_values = pixel_values.to(device)\n",
    "\n",
    "task_prompt = \"<s_menu>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = decoder_input_ids.to(device)\n",
    "outputs = model.generate(\n",
    "    pixel_values,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "seq = processor.batch_decode(outputs.sequences)[0]\n",
    "seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "seq = processor.token2json(seq)\n",
    "print(seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
